# LEXICAL_ANLYSER
A Lexical Analyser is the preliminary phase of a compiler that performs lexical analysis, transforming a raw character stream into a structured sequence of tokens. It eliminates extraneous symbols (whitespace, comments), detects lexemes, classifies them into token categories, and furnishes these to the syntax analyser for higher-level parsing.


🔤 Lexical Analyser

-> The Lexical Analyser project is a simple compiler design tool built using C language.
-> It scans the source code (input program) and breaks it into meaningful tokens such as keywords, identifiers, operators, literals, and symbols.
This project introduces fundamental concepts of compiler construction, including lexical analysis, tokenization, and file handling in C.

📒 Lexical Analyser in C

-> The Lexical Analyser (or Lexer) is the first phase of a compiler.
-> It reads the input program character by character and converts it into a sequence of tokens, which are later passed to the parser for syntax analysis.
-> This project demonstrates how token recognition works using C programming, string handling, and file handling concepts.

📌 This project is useful for understanding concepts of

-> C programming

-> Compiler Design fundamentals

-> Lexical Analysis process

-> String and character processing in C

-> File handling (reading source code from a file)

Modular programming

✨ Features

-> 📑 Tokenization: Reads input source code and splits it into tokens

-> 🔠 Keyword Recognition: Detects and classifies C keywords (e.g., int, float, if, while)

-> 🆔 Identifier Recognition: Distinguishes variables, function names, etc.

-> ➕ Operator Recognition: Identifies arithmetic, relational, and logical operators

-> 🔢 Constant Recognition: Detects numeric constants and literals

-> 🏷️ Special Symbols: Recognizes symbols like ;, ,, (), {}

-> 💾 File-based Input: Source code is read from a text file (e.g., input.c) for analysis

-> 📋 Structured Output: Displays tokens with their type in a neat tabular format

-> 💻 How the System Works

The program reads source code from a file (e.g., program.c)

-> It scans the code character by character

-> It identifies valid tokens such as keywords, identifiers, operators, constants, and symbols

-> The recognized tokens are displayed with their respective categories

-> Errors or invalid tokens are flagged accordingly

🎯 Educational Value

. This project helps learners in C programming and compiler design to:

. Understand how lexical analysis works as the first phase of a compiler

. Practice string processing and pattern matching in C

. Work with file I/O operations (fopen, fgetc, fclose)

. Learn how to classify tokens systematically

. Gain hands-on experience in compiler construction basics

👨‍💻 Author

A Bharath Simha Reddy
